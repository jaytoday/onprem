{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from onprem.core import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Built-In Web App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**[OnPrem.LLM](https://github.com/amaiya/onprem)** includes a built-in web app to easily access and use LLMs. After [installing](https://github.com/amaiya/onprem#install) OnPrem.LLM, you can follow these steps to prepare the web app and start it:\n",
    "\n",
    "#### Step 1: Ingest some documents using the Python API:\n",
    "```python\n",
    "# run at Python prompt\n",
    "from onprem import LLM\n",
    "llm = LLM()\n",
    "llm.ingest('/your/folder/of/documents')\n",
    "```\n",
    "\n",
    "#### Step 2: Start the Web app:\n",
    "\n",
    "```shell\n",
    "# run at command-line\n",
    "onprem --port 8000\n",
    "```\n",
    "Then, enter `localhost:8000` (or `<domain_name>:8000` if running on remote server) in your Web browser to access the application:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/amaiya/onprem/master/images/onprem_screenshot.png\" border=\"1\" alt=\"screenshot\" width=\"775\"/>\n",
    "\n",
    "The Web app is implemented with [streamlit](https://streamlit.io/): `pip install streamlit`.  If it is not already installed, the `onprem` command will ask you to install it.\n",
    "Here is more information on the `onprem` command:\n",
    "```sh\n",
    "$:~/projects/github/onprem$ onprem --help\n",
    "usage: onprem [-h] [-p PORT] [-a ADDRESS] [-v]\n",
    "\n",
    "Start the OnPrem.LLM web app\n",
    "Example: onprem --port 8000\n",
    "\n",
    "optional arguments:\n",
    "  -h, --help            show this help message and exit\n",
    "  -p PORT, --port PORT  Port to use; default is 8501\n",
    "  -a ADDRESS, --address ADDRESS\n",
    "                        Address to bind; default is 0.0.0.0\n",
    "  -v, --version         Print a version\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The app requires a file called `webapp.yml` exists in the `onprem_data` folder in the user's home directory. This file stores information used by the Web app such as the model to use. If one does not exist, then a default one will be created for you and is also shown below:\n",
    "\n",
    "```yaml\n",
    "# Default YAML configuration\n",
    "llm:\r\n",
    "  # model url (or model file name if previously downloaded)\r\n",
    "  model_url: https://huggingface.co/TheBloke/WizardLM-13B-V1.2-GGUF/resolve/main/wizardlm-13b-v1.2.Q4_K_M.gguf\r\n",
    "  # number of layers offloaded to GPU\r\n",
    "  n_gpu_layers: 32\r\n",
    "  # path to vector db folder\r\n",
    "  vectordb_path: {datadir}/vectordb\r\n",
    "  # path to model download folder\r\n",
    "  model_download_path: {datadir}\r\n",
    "  # number of source documents used by LLM.ask and LLM.chat\r\n",
    "  rag_num_source_docs: 6\r\n",
    "  # minimum similarity score for source to be considered by LLM.ask/LLM.chat\r\n",
    "  rag_score_threshold: 0.0\r\n",
    "  # verbosity of Llama.cpp\r\n",
    "  verbose: TRUE\r\n",
    "prompt:\r\n",
    "  # prompt_template used with LLM.prompt (e.g, for models that accept a system prompt)\r\n",
    "  prompt_template:\r\n",
    "ui:\r\n",
    "  # title of application\r\n",
    "  title: OnPrem.LLM\r\n",
    "  # subtitle in \"Talk to Your Documents\" screen\r\n",
    "  rag_title:\r\n",
    "  # path to markdown file with contents that will be inserted below rag_title\r\n",
    "  rag_text_path:\r\n",
    "  # path to folder containing raw documents (i.e., absolute path of folder you supplied to LLM.ingest)\r\n",
    "  rag_source_path:\r\n",
    "  # base url (leave blank unless you're running your own separate web server to serve source documents)\r\n",
    "  rag_base_url:\n",
    "```\n",
    "\n",
    "You can edit the file based on your requirements. Variables in the `llm` section are automatically passed to the `onprem.LLM` constructor, which, in turn, passes extra `**kwargs` to `llama-cpp-python`.  For instance, you can add a `temperature` variable in the `llm` section to adjust temperature of the model in the web app (e.g., lower values closer to 0.0 for more deterministic output and higher values for more creativity). \n",
    "\n",
    "The default model in the auto-created YAML file is a [13B parameter model](https://huggingface.co/TheBloke/WizardLM-13B-V1.2-GGUF/resolve/main/wizardlm-13b-v1.2.Q4_K_M.gguf).  If this is too large and slow for your system, you can edit `model_url` above to use a [7B parameter model](https://huggingface.co/TheBloke/Wizard-Vicuna-7B-Uncensored-GGUF/resolve/main/Wizard-Vicuna-7B-Uncensored.Q4_K_M.gguf) or [3B parameter model](https://huggingface.co/juanjgit/orca_mini_3B-GGUF/resolve/main/orca-mini-3b.q4_0.gguf) with faster speed at the expense of some performance. Of course, you can also edit `model_url` to use a larger model, as well. Any model\n",
    " in GGUF format can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that some models have particular prompt formats.  For instance, if using the **Zephyr-7B** model, as described on the [model's home page](https://huggingface.co/TheBloke/zephyr-7B-beta-GGUF#prompt-template-zephyr), the `prompt_template` in the YAML file must be set to:\n",
    "```yaml\n",
    "prompt:\n",
    "  prompt_template: <|system|>\\n</s>\\n<|user|>\\n{prompt}</s>\\n<|assistant|>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Talk To Your Documents\n",
    "The Web app has two screens.  The first screen (shown above) is a UI for [retrieval augmented generation](https://arxiv.org/abs/2005.11401) or RAG (i.e., chatting with documents). Sources considered by the LLM when generating answers are displayed and ranked by answer-to-source similarity. Hovering over the question marks in the sources will display the snippets of text from a document considered by the LLM when generating answers.\n",
    "\n",
    "**Hover Example:**\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/amaiya/onprem/master/images/hover-example.png\" border=\"1\" alt=\"screenshot\" width=\"775\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Source Hyperlinks:** On Linux and Mac systems where Python is installed in your home directory (e.g., `~/mambaforge`, `~/anaconda3`), displayed sources for the answer should automatically appear as hyperlinks to the original documents (e.g, PDFs, TXTs, etc.) if you populate the `rag_source_path` variable in `webapp.yml` with the the absolute path of the folder supplied to `LLM.ingest`.  You should leave `rag_base_url` blank in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Prompts to Solve Problems\n",
    "\n",
    "The second screen is a UI for general prompting and allows you to supply prompts to the LLM to solve problems.\n",
    "\n",
    "**Information Extraction Example:**\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/amaiya/onprem/master/images/extraction.png\" border=\"1\" alt=\"screenshot\" width=\"775\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have fun!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/amaiya/onprem/master/images/cat_names.png\" border=\"1\" alt=\"screenshot\" width=\"775\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
