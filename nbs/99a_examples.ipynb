{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from onprem.core import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Prompts to Solve Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In these examples of **[OnPrem.LLM](https://github.com/amaiya/onprem)**, we will accelerate inference using a GPU.  We use an NVIDIA Titan V GPU with a modest 12GB of VRAM.\n",
    "For GPU acceleration, make sure you installed `llama-cpp-python` with CUBLAS support as [described here](https://amaiya.github.io/onprem/#speeding-up-inference-using-a-gpu).\n",
    "\n",
    "\n",
    "After that, you just need to supply the `n_gpu_layers` argument to `LLM` for GPU-accelerated responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the `LLM` instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | notest\n",
    "from onprem import LLM\n",
    "import os\n",
    "\n",
    "llm = LLM(n_gpu_layers=35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cillian Murphy, Florence Pugh"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "prompt = \"\"\"Extract the names of people in the supplied sentences. Here is an example:\n",
    "Sentence: James Gandolfini and Paul Newman were great actors.\n",
    "People:\n",
    "James Gandolfini, Paul Newman\n",
    "Sentence:\n",
    "I like Cillian Murphy's acting. Florence Pugh is great, too.\n",
    "People:\"\"\"\n",
    "\n",
    "saved_output = llm.prompt(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more complicated example of **Information Extraction**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Norton Schwartz | Position: President| Company: Institute for Defense Analyses"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "\n",
    "prompt = \"\"\"Extract the Name, Position, and Company from the following sentences.  Here are some examples.\n",
    "\n",
    "Text: Alan F. Estevez serves as the Under Secretary of Commerce for Industry and Security.  As Under Secretary, Mr. Estevez leads \n",
    "the Bureau of Industry and Security, which advances U.S. national security, foreign policy, and economic objectives by ensuring an \n",
    "effective export control and treaty compliance system and promoting U.S. strategic technology leadership.\n",
    "Name:  Alan F. Estevez | Position: Under Secretary | Company: Bureau of Industry and Security\n",
    "###\n",
    "Text: Pichai Sundararajan (born June 10, 1972[3][4][5]), better known as Sundar Pichai (/ˈsʊndɑːr pɪˈtʃaɪ/), is an Indian-born American \n",
    "business executive.[6][7] He is the chief executive officer (CEO) of Alphabet Inc. and its subsidiary Google.[8]\n",
    "Name:   Sundar Pichai | Position: CEO | Company: Google\n",
    "###\n",
    "Text: Norton Allan Schwartz (born December 14, 1951)[1] is a retired United States Air Force general[2] who served as the 19th Chief of Staff of the \n",
    "Air Force from August 12, 2008, until his retirement in 2012.[3] He previously served as commander, United States Transportation Command from \n",
    "September 2005 to August 2008. He is currently the president of the Institute for Defense Analyses, serving since January 2, 2020.[4]\n",
    "\"\"\"\n",
    "saved_output = llm.prompt(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grammar Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I don't want to go."
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "\n",
    "prompt = \"\"\"Correct the grammar and spelling in the supplied sentences.  Here are some examples.\n",
    "[Sentence]:\n",
    "I love goin to the beach.\n",
    "[Correction]: I love going to the beach.\n",
    "[Sentence]:\n",
    "Let me hav it!\n",
    "[Correction]: Let me have it!\n",
    "[Sentence]:\n",
    "It have too many drawbacks.\n",
    "[Correction]: It has too many drawbacks.\n",
    "[Sentence]:\n",
    "I do not wan to go\n",
    "[Correction]:\"\"\"\n",
    "saved_output = llm.prompt(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Positive"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "\n",
    "prompt = \"\"\"Classify each sentence as either positive, negative, or neutral.  Here are some examples.\n",
    "[Sentence]: I love going to the beach.\n",
    "[[Classification]: Positive\n",
    "[Sentence]: It is 10am right now.\n",
    "[Classification]: Neutral\n",
    "[Sentence]: I just got fired from my job.\n",
    "[Classification]: Negative\n",
    "[Sentence]: The reactivity of  your team has been amazing, thanks!\n",
    "[Classification]:\"\"\"\n",
    "\n",
    "saved_output = llm.prompt(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paraphrasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paraphrase: Following a prolonged war in Afghanistan that lasted for 20 years, when both U.S. Presidents Trump and Biden decided to withdraw American troops from the country, Kabul - the capital city of Afghanistan - fell within hours into the hands of the Taliban without any resistance."
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "\n",
    "prompt = \"\"\"Paraphrase the following text delimited by triple backticks. \n",
    "```After a war lasting 20 years, following the decision taken first by President Trump and then by President Biden to withdraw American troops, Kabul, the capital of Afghanistan, fell within a few hours to the Taliban, without resistance.```\n",
    "\"\"\"\n",
    "saved_output = llm.prompt(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-Shot Answer Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "GPU Plan"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "\n",
    "prompt = \"\"\"Answer the Question based on the Context.  Here are some examples.\n",
    "[Context]: \n",
    "NLP Cloud was founded in 2021 when the team realized there was no easy way to reliably leverage Natural Language Processing in production.\n",
    "Question: When was NLP Cloud founded?\n",
    "[Answer]: \n",
    "2021\n",
    "###\n",
    "[Context]:\n",
    "NLP Cloud developed their API by mid-2020 and they added many pre-trained open-source models since then.\n",
    "[Question]: \n",
    "What did NLP Cloud develop?\n",
    "[Answer]:\n",
    "API\n",
    "###\n",
    "[Context]:\n",
    "All plans can be stopped anytime. You only pay for the time you used the service. In case of a downgrade, you will get a discount on your next invoice.\n",
    "[Question]:\n",
    "When can plans be stopped?\n",
    "[Answer]:\n",
    "Anytime\n",
    "###\n",
    "[Context]:\n",
    "The main challenge with GPT-J is memory consumption. Using a GPU plan is recommended.\n",
    "[Question]:\n",
    "Which plan is recommended for GPT-J?\n",
    "Answer:\"\"\"\n",
    "saved_output = llm.prompt(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Product Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A stylish and comfortable t-shirt for men, for just $39.\n"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "\n",
    "prompt = \"\"\"Generate a Sentence from the Keywords. Here are some examples.\n",
    "[Keywords]:\n",
    "shoes, women, $59\n",
    "[Sentence]:\n",
    "Beautiful shoes for women at the price of $59.\n",
    "###\n",
    "[Keywords]:\n",
    "trousers, men, $69\n",
    "[Sentence]:\n",
    "Modern trousers for men, for $69 only.\n",
    "###\n",
    "[Keywords]:\n",
    "gloves, winter, $19\n",
    "[Sentence]: \n",
    "Amazingly hot gloves for cold winters, at $19.\n",
    "###\n",
    "[Keywords]: \n",
    "t-shirt, men, $39\n",
    "[Sentence]:\"\"\"\n",
    "saved_output = llm.prompt(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The climate is changing, but our actions can save it from catastrophe.\n",
      "###"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "prompt = \"\"\"Generate a tweet based on the supplied Keyword. Here are some examples.\n",
    "[Keyword]:\n",
    "markets\n",
    "[Tweet]:\n",
    "Take feedback from nature and markets, not from people\n",
    "###\n",
    "[Keyword]:\n",
    "children\n",
    "[Tweet]:\n",
    "Maybe we die so we can come back as children.\n",
    "###\n",
    "[Keyword]:\n",
    "startups\n",
    "[Tweet]: \n",
    "Startups should not worry about how to put out fires, they should worry about how to start them.\n",
    "###\n",
    "[Keyword]:\n",
    "climate change\n",
    "[Tweet]:\"\"\"\n",
    "saved_output = llm.prompt(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drafting Emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Subject: Introducing Tesla, the future of transportation\n",
      "Dear Shareholder,\n",
      "I am writing to introduce you to Tesla, the leading manufacturer of electric vehicles (EVs) and energy storage products. \n",
      "Tesla has been at the forefront of innovating EV technology since its inception in 2003. With a history of creating breakthrough products such as the Tesla Model S, Tesla Powerwall, and Solar Roof, it is clear that Tesla's mission is to create a better future through sustainable transportation and energy solutions.\n",
      "Tesla has experienced rapid growth in recent years, with its stock price increasing by 475% since January 2016. This growth can be attributed to several factors, including the increase in global interest in sustainability and environmental issues, as well as Tesla's continuous innovation and success in the EV market.\n",
      "Overall, Tesla is a company that not only produces cutting-edge products but also has a strong commitment to sustainability and innovation. As a shareholder of Tesla, I encourage you to take advantage of this opportunity to invest in the future of transportation and energy storage solutions with Tesla.\n",
      "Thank you for your continued support of Tesla."
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "prompt = \"\"\"Generate an email introducing Tesla to shareholders.\"\"\"\n",
    "saved_output = llm.prompt(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Talk to Your Documents\n",
    "\n",
    "**Note:**  Although we are using the default 7B-parameter model in the example below, we recommend using a larger model for retrieval-augmented question-answering use cases like this in order to reduce hallucinations.  You can use the default 13B-parameter model by supplying `use_larger=True` in the call to `onprem.LLM`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-05 14:47:43.265584: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new vectorstore\n",
      "Loading documents from ./sample_data/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading new documents: 100%|██████████████████████| 2/2 [00:00<00:00, 16.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 11 new documents from ./sample_data/\n",
      "Split into 62 chunks of text (max. 500 chars each)\n",
      "Creating embeddings. May take some minutes...\n",
      "Ingestion complete! You can now query your documents using the LLM.ask method\n"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "llm.ingest(\"./sample_data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " KTrain is a low-code library for augmented machine learning that automates or semi-automates various aspects of the ML workow, from data preprocessing to model training and application. It is inspired by other low-code and no-code open-source ML libraries such as fastai and ludwig, and is intended to help democratize machine learning by enabling beginners and domain experts with minimal programming or data science expertise."
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "\n",
    "result = llm.ask(\"What is ktrain? Remember to only use the provided context.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pro-Tip**: If you see the model hallucinating answers, you can supply `use_larger=True` to `onprem.LLM` and use the slightly larger default model better-suited to this use case (or supply the URL to a different model of your choosing to `LLM`), which can provide better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
